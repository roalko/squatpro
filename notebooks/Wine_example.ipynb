{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine \n",
    "import pandas as pd\n",
    "\n",
    "wine_data = load_wine()\n",
    "\n",
    "X = wine_data.data\n",
    "\n",
    "y = wine_data.target\n",
    "\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index     128\n",
       "0        1424\n",
       "1        1424\n",
       "2        1424\n",
       "3        1424\n",
       "4        1424\n",
       "5        1424\n",
       "6        1424\n",
       "7        1424\n",
       "8        1424\n",
       "9        1424\n",
       "10       1424\n",
       "11       1424\n",
       "12       1424\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].astype('float16').memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_numeric(X[0],downcast=\"float\").memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14.49857941324234235124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>12.84</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.61</td>\n",
       "      <td>24.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.81</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.15</td>\n",
       "      <td>590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>13.69</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.54</td>\n",
       "      <td>20.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.82</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>13.51</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.54</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>12.36</td>\n",
       "      <td>3.83</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.04</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.58</td>\n",
       "      <td>520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>13.41</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.12</td>\n",
       "      <td>18.8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1035.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>13.03</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.71</td>\n",
       "      <td>16.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.46</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.48</td>\n",
       "      <td>392.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>13.52</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.72</td>\n",
       "      <td>23.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.06</td>\n",
       "      <td>520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.30</td>\n",
       "      <td>23.6</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.07</td>\n",
       "      <td>3.21</td>\n",
       "      <td>625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>11.64</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.46</td>\n",
       "      <td>21.6</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3      4     5     6     7     8     9     10    11  \\\n",
       "139  12.84  2.96  2.61  24.0  101.0  2.32  0.60  0.53  0.81  4.92  0.89  2.15   \n",
       "3    14.37  1.95  2.50  16.8  113.0  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "161  13.69  3.26  2.54  20.0  107.0  1.83  0.56  0.50  0.80  5.88  0.96  1.82   \n",
       "34   13.51  1.80  2.65  19.0  110.0  2.35  2.53  0.29  1.54  4.20  1.10  2.87   \n",
       "160  12.36  3.83  2.38  21.0   88.0  2.30  0.92  0.50  1.04  7.65  0.56  1.58   \n",
       "..     ...   ...   ...   ...    ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "41   13.41  3.84  2.12  18.8   90.0  2.45  2.68  0.27  1.48  4.28  0.91  3.00   \n",
       "76   13.03  0.90  1.71  16.0   86.0  1.95  2.03  0.24  1.46  4.60  1.19  2.48   \n",
       "142  13.52  3.17  2.72  23.5   97.0  1.55  0.52  0.50  0.55  4.35  0.89  2.06   \n",
       "89   12.08  1.33  2.30  23.6   70.0  2.20  1.59  0.42  1.38  1.74  1.07  3.21   \n",
       "88   11.64  2.06  2.46  21.6   84.0  1.95  1.69  0.48  1.35  2.80  1.00  2.75   \n",
       "\n",
       "         12  \n",
       "139   590.0  \n",
       "3    1480.0  \n",
       "161   680.0  \n",
       "34   1095.0  \n",
       "160   520.0  \n",
       "..      ...  \n",
       "41   1035.0  \n",
       "76    392.0  \n",
       "142   520.0  \n",
       "89    625.0  \n",
       "88    680.0  \n",
       "\n",
       "[142 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>11.46</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.82</td>\n",
       "      <td>19.5</td>\n",
       "      <td>107.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.24</td>\n",
       "      <td>3.58</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.81</td>\n",
       "      <td>562.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>12.33</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.67</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>11.56</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.23</td>\n",
       "      <td>28.5</td>\n",
       "      <td>119.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.87</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.69</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>12.00</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.05</td>\n",
       "      <td>564.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>13.08</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.36</td>\n",
       "      <td>21.5</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>9.40</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.33</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>12.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>17.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.96</td>\n",
       "      <td>710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>12.45</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.64</td>\n",
       "      <td>27.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.14</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.73</td>\n",
       "      <td>880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>12.37</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.10</td>\n",
       "      <td>18.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.95</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.77</td>\n",
       "      <td>660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>13.07</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>11.96</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.30</td>\n",
       "      <td>21.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3.13</td>\n",
       "      <td>886.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>12.22</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.94</td>\n",
       "      <td>19.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.02</td>\n",
       "      <td>312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>12.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.29</td>\n",
       "      <td>630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>12.93</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>21.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.31</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>12.51</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.98</td>\n",
       "      <td>20.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.57</td>\n",
       "      <td>672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>12.16</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.31</td>\n",
       "      <td>22.8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.26</td>\n",
       "      <td>495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>11.82</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.88</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.44</td>\n",
       "      <td>415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>12.51</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.25</td>\n",
       "      <td>17.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.51</td>\n",
       "      <td>650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>12.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.12</td>\n",
       "      <td>19.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>13.58</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.36</td>\n",
       "      <td>19.1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.95</td>\n",
       "      <td>6.90</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>13.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.42</td>\n",
       "      <td>14.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.87</td>\n",
       "      <td>7.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>11.81</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.74</td>\n",
       "      <td>21.5</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.26</td>\n",
       "      <td>625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>12.99</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.96</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.50</td>\n",
       "      <td>985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.19</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.48</td>\n",
       "      <td>16.5</td>\n",
       "      <td>108.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.86</td>\n",
       "      <td>8.70</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>13.58</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.69</td>\n",
       "      <td>24.5</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.54</td>\n",
       "      <td>8.66</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.80</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>11.61</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.70</td>\n",
       "      <td>20.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.26</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>13.56</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.46</td>\n",
       "      <td>20.5</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.45</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>12.17</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.53</td>\n",
       "      <td>19.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.23</td>\n",
       "      <td>355.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>12.07</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.28</td>\n",
       "      <td>378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>12.21</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.75</td>\n",
       "      <td>16.8</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.07</td>\n",
       "      <td>718.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14.22</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>13.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.08</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.53</td>\n",
       "      <td>760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.87</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>19.4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.40</td>\n",
       "      <td>915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>12.70</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.40</td>\n",
       "      <td>23.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.13</td>\n",
       "      <td>463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>13.05</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.04</td>\n",
       "      <td>12.4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2.91</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.64</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.56</td>\n",
       "      <td>15.2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.66</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.36</td>\n",
       "      <td>845.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3      4     5     6     7     8     9     10    11  \\\n",
       "4    13.24  2.59  2.87  21.0  118.0  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "110  11.46  3.74  1.82  19.5  107.0  3.18  2.58  0.24  3.58  2.90  0.75  2.81   \n",
       "60   12.33  1.10  2.28  16.0  101.0  2.05  1.09  0.63  0.41  3.27  1.25  1.67   \n",
       "121  11.56  2.05  3.23  28.5  119.0  3.18  5.08  0.47  1.87  6.00  0.93  3.69   \n",
       "119  12.00  3.43  2.00  19.0   87.0  2.00  1.64  0.37  1.87  1.28  0.93  3.05   \n",
       "176  13.17  2.59  2.37  20.0  120.0  1.65  0.68  0.53  1.46  9.30  0.60  1.62   \n",
       "149  13.08  3.90  2.36  21.5  113.0  1.41  1.39  0.34  1.14  9.40  0.57  1.33   \n",
       "100  12.08  2.08  1.70  17.5   97.0  2.23  2.17  0.26  1.40  3.30  1.27  2.96   \n",
       "157  12.45  3.03  2.64  27.0   97.0  1.90  0.58  0.63  1.14  7.50  0.67  1.73   \n",
       "98   12.37  1.07  2.10  18.5   88.0  3.52  3.75  0.24  1.95  4.50  1.04  2.77   \n",
       "38   13.07  1.50  2.10  15.5   98.0  2.40  2.64  0.28  1.37  3.70  1.18  2.69   \n",
       "74   11.96  1.09  2.30  21.0  101.0  3.38  2.14  0.13  1.65  3.21  0.99  3.13   \n",
       "108  12.22  1.29  1.94  19.0   92.0  2.36  2.04  0.39  2.08  2.70  0.86  3.02   \n",
       "130  12.86  1.35  2.32  18.0  122.0  1.51  1.25  0.21  0.94  4.10  0.76  1.29   \n",
       "140  12.93  2.81  2.70  21.0   96.0  1.54  0.50  0.53  0.75  4.60  0.77  2.31   \n",
       "104  12.51  1.73  1.98  20.5   85.0  2.20  1.92  0.32  1.48  2.94  1.04  3.57   \n",
       "86   12.16  1.61  2.31  22.8   90.0  1.78  1.69  0.43  1.56  2.45  1.33  2.26   \n",
       "103  11.82  1.72  1.88  19.5   86.0  2.50  1.64  0.37  1.42  2.06  0.94  2.44   \n",
       "134  12.51  1.24  2.25  17.5   85.0  2.00  0.58  0.60  1.25  5.45  0.75  1.51   \n",
       "106  12.25  1.73  2.12  19.0   80.0  1.65  2.03  0.37  1.63  3.40  1.00  3.17   \n",
       "31   13.58  1.66  2.36  19.1  106.0  2.86  3.19  0.22  1.95  6.90  1.09  2.88   \n",
       "52   13.82  1.75  2.42  14.0  111.0  3.88  3.74  0.32  1.87  7.05  1.01  3.26   \n",
       "96   11.81  2.12  2.74  21.5  134.0  1.60  0.99  0.14  1.56  2.50  0.95  2.26   \n",
       "73   12.99  1.67  2.60  30.0  139.0  3.30  2.89  0.21  1.96  3.35  1.31  3.50   \n",
       "18   14.19  1.59  2.48  16.5  108.0  3.30  3.93  0.32  1.86  8.70  1.23  2.82   \n",
       "168  13.58  2.58  2.69  24.5  105.0  1.55  0.84  0.39  1.54  8.66  0.74  1.80   \n",
       "109  11.61  1.35  2.70  20.0   94.0  2.74  2.92  0.29  2.49  2.65  0.96  3.26   \n",
       "55   13.56  1.73  2.46  20.5  116.0  2.96  2.78  0.20  2.45  6.25  0.98  3.03   \n",
       "64   12.17  1.45  2.53  19.0  104.0  1.89  1.75  0.45  1.03  2.95  1.45  2.23   \n",
       "125  12.07  2.16  2.17  21.0   85.0  2.60  2.65  0.37  1.35  2.76  0.86  3.28   \n",
       "69   12.21  1.19  1.75  16.8  151.0  1.85  1.28  0.14  2.50  2.85  1.28  3.07   \n",
       "39   14.22  3.99  2.51  13.2  128.0  3.00  3.04  0.20  2.08  5.10  0.89  3.53   \n",
       "28   13.87  1.90  2.80  19.4  107.0  2.95  2.97  0.37  1.76  4.50  1.25  3.40   \n",
       "79   12.70  3.87  2.40  23.0  101.0  2.83  2.55  0.43  1.95  2.57  1.19  3.13   \n",
       "50   13.05  1.73  2.04  12.4   92.0  2.72  3.27  0.17  2.91  7.20  1.12  2.91   \n",
       "19   13.64  3.10  2.56  15.2  116.0  2.70  3.03  0.17  1.66  5.10  0.96  3.36   \n",
       "\n",
       "         12  \n",
       "4     735.0  \n",
       "110   562.0  \n",
       "60    680.0  \n",
       "121   465.0  \n",
       "119   564.0  \n",
       "176   840.0  \n",
       "149   550.0  \n",
       "100   710.0  \n",
       "157   880.0  \n",
       "98    660.0  \n",
       "38   1020.0  \n",
       "74    886.0  \n",
       "108   312.0  \n",
       "130   630.0  \n",
       "140   600.0  \n",
       "104   672.0  \n",
       "86    495.0  \n",
       "103   415.0  \n",
       "134   650.0  \n",
       "106   510.0  \n",
       "31   1515.0  \n",
       "52   1190.0  \n",
       "96    625.0  \n",
       "73    985.0  \n",
       "18   1680.0  \n",
       "168   750.0  \n",
       "109   680.0  \n",
       "55   1120.0  \n",
       "64    355.0  \n",
       "125   378.0  \n",
       "69    718.0  \n",
       "39    760.0  \n",
       "28    915.0  \n",
       "79    463.0  \n",
       "50   1150.0  \n",
       "19    845.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_centered = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 1)\n",
    "Xpc = pca.fit_transform(X_train_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "cross_val_score(model, Xpc, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xpc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_centered = scaler.transform(X_test)\n",
    "\n",
    "X_test_pc = pca.transform(X_test_centered)\n",
    "\n",
    "pred = model.predict(X_test_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"This model got\", accuracy_score(y_test, pred)*100, \"% of predictions right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First New Thing: saving your model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model,'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_model = joblib.load('model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = good_model.predict(X_test_pc)\n",
    "\n",
    "print(\"This model got\", accuracy_score(y_test, new_pred)*100, \"% of predictions right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repeatable flow with Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "wine_data = load_wine()\n",
    "\n",
    "X = wine_data.data\n",
    "\n",
    "y = wine_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', StandardScaler()),('pca',PCA()),('model',LogisticRegression())],memory=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(pca__n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe,X_train,y_train,cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipe,'pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pipeline = joblib.load('pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Repeatable flow with Pipelines Generalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "wine_data = load_wine()\n",
    "\n",
    "X = wine_data.data\n",
    "\n",
    "y = wine_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,n_components):\n",
    "    pipe = Pipeline(steps=[('scaler', StandardScaler()),('pca',PCA()),('model',model)],memory=\"local_path\")\n",
    "    pipe.set_params(pca__n_components=n_components)\n",
    "    mean_cv = cross_val_score(pipe,X_train,y_train,cv=10).mean()\n",
    "    print(f'model got {mean_cv} Cross-Validation score')\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [LogisticRegression(),DecisionTreeClassifier(),SVC()]:\n",
    "    train(model,n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = train(LogisticRegression(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Way of iterating through models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_pipe = Pipeline(steps=[('scaler', StandardScaler()),('pca',PCA()),('model',model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { 'pca__n_components': [1, 2, 3, 4, 5],\n",
    "    'model': [LogisticRegression(),DecisionTreeClassifier(),SVC()]}\n",
    "\n",
    "search = GridSearchCV(other_pipe, param_grid, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "print(f\"Best parameters (CV score={search.best_score_}):\")\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe =  Pipeline(steps=[('scaler', StandardScaler()),('pca',PCA(n_components=5)),('model',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_pipe,'best_pipe.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Repeatable flow with Pipelines even more generalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class Trainer(object):\n",
    "    \n",
    "    def __init__(self, X, y, **kwargs):\n",
    "        self.pipeline = None\n",
    "        self.kwargs = kwargs\n",
    "        self.training_error = None\n",
    "        self.test_error = None\n",
    "        \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val =\\\n",
    "        train_test_split(X, y, test_size=self.kwargs.get(\"test_size\"))\n",
    "        \n",
    "        self.alive()\n",
    "        \n",
    "    def alive(self):\n",
    "        print(\"I'm alive!\")   \n",
    "\n",
    "    def get_estimator(self):\n",
    "        estimator = self.kwargs.get(\"estimator\")\n",
    "        if estimator == \"LogisticRegression\":\n",
    "            model = LogisticRegression(**self.kwargs.get(\"model_params\"))\n",
    "        if estimator == \"SVC\":\n",
    "            model = SVC(**self.kwargs.get(\"model_params\"))\n",
    "        return model\n",
    "    \n",
    "    def set_pipeline(self):   \n",
    "        steps = []\n",
    "            \n",
    "        if self.kwargs.get(\"scale_data\"):\n",
    "            steps.append('sacler', StandardScaler())\n",
    "        if self.kwargs.get(\"pca\"):\n",
    "             steps.append(('pca',PCA()))\n",
    "                \n",
    "        steps.append(('model',self.get_estimator()))\n",
    "            \n",
    "        self.pipeline = Pipeline(steps)\n",
    "        \n",
    "        if self.kwargs.get(\"pca_params\"):\n",
    "            self.pipeline.set_params(**self.kwargs.get(\"pca_params\"))\n",
    "        \n",
    "    def train(self):\n",
    "        self.set_pipeline()\n",
    "        self.pipeline.fit(self.X_train, self.y_train)\n",
    "        mean_cv = cross_val_score(self.pipeline,self.X_train,self.y_train,cv=10).mean()\n",
    "        print(f'model got {mean_cv} Cross-Validation score')\n",
    "        self.training_error = mean_cv\n",
    "        \n",
    "    def evaluate(self):\n",
    "        accuracy = self.pipeline.score(self.X_val,self.y_val)\n",
    "        print(f'model got {accuracy} Test Set score')\n",
    "        self.test_error = accuracy\n",
    "        \n",
    "    def save_model(self,filename):\n",
    "        joblib.dump(self.pipeline,filename)\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(test_size=0.2,\n",
    "              pca=True,\n",
    "              scaler=True,\n",
    "              estimator=\"SVC\",\n",
    "              pca_params={'pca__n_components':13},\n",
    "              model_params={'kernel':'linear'})\n",
    "\n",
    "my_trainer = Trainer(X,y,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_trainer = Trainer(X,y,test_size=0.2,pca=True,scaler=True,estimator=\"SVC\",pca_params={'pca__n_components':13},model_params={'kernel':'linear'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_trainer = Trainer(X,y,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_trainer.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer.training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer.save_model('SVM_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sharing experiments with others: MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  mlflow.tracking import MlflowClient\n",
    "\n",
    "EXPERIMENT_NAME = \"cool_experiment_woop_woop\"\n",
    "client = MlflowClient()\n",
    "experiment_id = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "for model in [\"linear\", \"Randomforest\"]:\n",
    "    run = client.create_run(experiment_id)\n",
    "    client.log_metric(run.info.run_id, \"rmse\", 4.5)\n",
    "    client.log_param(run.info.run_id, \"model\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"nice_experiment\"\n",
    "client = MlflowClient()\n",
    "experiment_id = client.create_experiment(EXPERIMENT_NAME)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {\"LogisticRegression\":{\"solver\":\"liblinear\"}, \"SVC\":{\"kernel\":\"linear\"}}\n",
    "\n",
    "for model in [\"LogisticRegression\", \"SVC\"]:\n",
    "\n",
    "    run = client.create_run(experiment_id)\n",
    "    \n",
    "    params = dict(test_size=0.2,\n",
    "              pca=True,\n",
    "              scaler=True,\n",
    "              estimator=model,\n",
    "              pca_params={'pca__n_components':13},\n",
    "              model_params=all_params.get(model))\n",
    "    \n",
    "    my_trainer = Trainer(X,y,**params)\n",
    "    \n",
    "    my_trainer.train()\n",
    "    my_trainer.evaluate()\n",
    "        \n",
    "    client.log_metric(run.info.run_id, \"training_error\", my_trainer.training_error)\n",
    "    client.log_metric(run.info.run_id, \"test_error\", my_trainer.test_error)\n",
    "    client.log_param(run.info.run_id, \"model\", params.get(\"estimator\"))\n",
    "    client.log_param(run.info.run_id, \"pca_components\", params.get(\"pca_params\"))\n",
    "    client.log_param(run.info.run_id, \"model_params\", params.get(\"model_params\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoized property demo\n",
    "\n",
    "Memoized propeties are a trick you can use to \"freeze\" variables, values returned by methods or attributes of an object that otherwise changes everytime you instantiate the object or run a method.\n",
    "\n",
    "See the example below. If you run the example with @memoized_property uncommented, you will notice that the value retuned <code>get_random_value</code> will no longer change if you call it again with the same \"Car\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memoized_property import memoized_property\n",
    "from numpy import random\n",
    "\n",
    "class Car():\n",
    "    @memoized_property\n",
    "    def get_random_value(self):\n",
    "        return random.randint(0,10)\n",
    "\n",
    "car = Car()\n",
    "print('non memoized calls differ:')\n",
    "print(car.get_random_value)\n",
    "print(car.get_random_value)\n",
    "\n",
    "car2 = Car()\n",
    "print('non memoized calls differ:')\n",
    "print(car2.get_random_value)\n",
    "print(car2.get_random_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from  mlflow.tracking import MlflowClient\n",
    "from memoized_property import memoized_property\n",
    "\n",
    "class Trainer(object):\n",
    "    \n",
    "    #MLFLOW_URI = \"https://mlflow.lewagon.co/\"\n",
    "    \n",
    "    def __init__(self, X, y,experiment_name, **kwargs):\n",
    "        self.pipeline = None\n",
    "        self.kwargs = kwargs\n",
    "        self.training_error = None\n",
    "        self.test_error = None\n",
    "        self.experiment_name = experiment_name\n",
    "        \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val =\\\n",
    "        train_test_split(X, y, test_size=self.kwargs.get(\"test_size\"))\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_client(self):\n",
    "        #mlflow.set_tracking_uri(self.MLFLOW_URI)\n",
    "        return MlflowClient()\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_experiment_id(self):\n",
    "        try:\n",
    "            return self.mlflow_client \\\n",
    "                .create_experiment(self.experiment_name)\n",
    "        except BaseException:\n",
    "            return self.mlflow_client \\\n",
    "                .get_experiment_by_name(self.experiment_name).experiment_id\n",
    "\n",
    "    def mlflow_create_run(self):\n",
    "        self.mlflow_run = self.mlflow_client \\\n",
    "            .create_run(self.mlflow_experiment_id)\n",
    "\n",
    "    def mlflow_log_param(self, key, value):\n",
    "        self.mlflow_client \\\n",
    "            .log_param(self.mlflow_run.info.run_id, key, value)\n",
    "\n",
    "    def mlflow_log_metric(self, key, value):\n",
    "        self.mlflow_client \\\n",
    "            .log_metric(self.mlflow_run.info.run_id, key, value)\n",
    "\n",
    "\n",
    "    def get_estimator(self):\n",
    "        estimator = self.kwargs.get(\"estimator\")\n",
    "        if estimator == \"LogisticRegression\":\n",
    "            model = LogisticRegression(**self.kwargs.get(\"model_params\"))\n",
    "        if estimator == \"SVC\":\n",
    "            model = SVC(**self.kwargs.get(\"model_params\"))\n",
    "        return model\n",
    "    \n",
    "    def set_pipeline(self):   \n",
    "        steps = []\n",
    "            \n",
    "        if self.kwargs.get(\"scale_data\"):\n",
    "            steps.append('sacler', StandardScaler())\n",
    "        if self.kwargs.get(\"pca\"):\n",
    "             steps.append(('pca',PCA()))\n",
    "                \n",
    "        steps.append(('model',self.get_estimator()))\n",
    "            \n",
    "        self.pipeline = Pipeline(steps)\n",
    "        \n",
    "        if self.kwargs.get(\"pca_params\"):\n",
    "            self.pipeline.set_params(**self.kwargs.get(\"pca_params\"))\n",
    "        \n",
    "    def train(self):\n",
    "        self.set_pipeline()\n",
    "        self.pipeline.fit(self.X_train, self.y_train)\n",
    "        mean_cv = cross_val_score(self.pipeline,self.X_train,self.y_train,cv=10).mean()\n",
    "        print(f'model got {mean_cv} Cross-Validation score')\n",
    "        self.training_error = mean_cv\n",
    "        \n",
    "        self.mlflow_create_run()\n",
    "        self.mlflow_log_metric(\"training_error\", mean_cv)\n",
    "        self.mlflow_log_param(\"model\", self.kwargs.get(\"estimator\"))\n",
    "        \n",
    "    def evaluate(self):\n",
    "        accuracy = self.pipeline.score(self.X_val,self.y_val)\n",
    "        print(f'model got {accuracy} Test Set score')\n",
    "        self.test_error = accuracy\n",
    "        \n",
    "        self.mlflow_log_metric(\"test_error\", accuracy)\n",
    "        \n",
    "    def save_model(self,filename):\n",
    "        joblib.dump(self.pipeline,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'mother_of_all_experiments'\n",
    "\n",
    "my_trainer = Trainer(X,y,experiment_name,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
