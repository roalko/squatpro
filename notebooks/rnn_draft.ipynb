{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------\n",
    "#THESE COULD BE TANGENTIAL TO A PIPELINE, BUT I HAVE WRITTEN A FUNCTION THAT LEVERAGES THEM BELOW\n",
    "--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_into_nested_list(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    list_of_lists = df.values.tolist()\n",
    "    return list_of_lists\n",
    "\n",
    "\n",
    "def store_X(list_of_files, X_list):\n",
    "    for fi in list_of_files:\n",
    "        X_list.append(convert_csv_into_nested_list(os.path.join(path, fi)))\n",
    "    return X_list\n",
    "        \n",
    "        \n",
    "def convert_X_to_ndarray(X, list_object):\n",
    "    for arr in list_object:\n",
    "        X.append(np.array(arr))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------\n",
    "#THIS GUY IS FINNICKY, LET ME KNOW IF YOU SEE SOMETHING WRONG WITH MY CODE\n",
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_values(X):\n",
    "    X_pad = pad_sequences(X, dtype='float32',padding='post',value=-1000)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------\n",
    "#NOTES\n",
    "-------\n",
    "\n",
    "#Robyn's functions for preprocessing\n",
    "#Run third cell and all functions execute\n",
    "#stores csvs in Features folder\n",
    "\n",
    "\n",
    "#Function that takes in three coordinates as input and finds the angle between those three points\n",
    "#note: there are points that were the exact position which created nan values for angles. example row 19 column 1 & 2 of 1.csv\n",
    "\n",
    "def create_angle(x1, y1, x2, y2, x3, y3):\n",
    "  vector_1 = [(x1-x2), (y1-y2)]\n",
    "  vector_2 = [(x3-x2), (y3-y2)] \n",
    "  if x1 == x2 and y1 == y2:\n",
    "    vector_1 = [1,0]\n",
    "  if x3 == x2 and y3 == y2:\n",
    "    vector_2 = [1,0]\n",
    "  unit_vector_1 = vector_1 / np.linalg.norm(vector_1)\n",
    "  unit_vector_2 = vector_2 / np.linalg.norm(vector_2)\n",
    "  dot_product = np.dot(unit_vector_1, unit_vector_2)\n",
    "  angle = np.arccos(dot_product)\n",
    "  #if angle = np.nan:\n",
    "  #  angle = padded_value(-1000)\n",
    "  return math.degrees(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that iterates through each row of a csv file and creates a list of angles\n",
    "#key1, key2, key3 are the columns for the x coordinates (example: column 0,2,4)\n",
    "\n",
    "def angles_list(csv_file, key_1, key_2, key_3):\n",
    "  df = pd.read_csv(csv_file)\n",
    "  list = [create_angle(df.loc[i][key_1], df.loc[i][key_1+1], df.loc[i][key_2], df.loc[i][key_2+1], df.loc[i][key_3], df.loc[i][key_3+1]) for i in range(len(df))]\n",
    "  return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that 1. calls angle_list function for each of the 100 CSV files  2. appends that list to a new column in the CSV\n",
    "def angle_column(list_key_1, list_key_2, list_key_3):\n",
    "  csv_files = '../raw_data/CSV_Files'\n",
    "  csv_features = '../raw_data/CSV_Features'\n",
    "  for file in os.listdir(csv_files):\n",
    "    #create a df from each csv file\n",
    "    df = pd.read_csv(os.path.join(csv_files, file))\n",
    "    i = 1\n",
    "    #loop over every CSV file in this folder and run angles_list function on it\n",
    "    for key_1, key_2, key_3 in zip(list_key_1, list_key_2, list_key_3):\n",
    "      list_of_angles = angles_list(os.path.join(csv_files, file), key_1, key_2, key_3)\n",
    "      #create a new column in the df and add each element of that list to that new df\n",
    "      df[f'feature_{i}'] = list_of_angles\n",
    "      i += 1    \n",
    "    #convert df back to csv and save csvs in a file called csv_features\n",
    "    df.to_csv(os.path.join(csv_features, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------------------------------\n",
    "#AUTOMATED CSV COLLECTION and X,y VARIABLE STORAGE\n",
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporary_preprocessing(good_or_training_path, bad_or_validation_path):\n",
    "    '''this pads and stores X and generates a matching y'''\n",
    "    \n",
    "    # get the good features or training data\n",
    "    X_list = []\n",
    "    X = []\n",
    "    list_of_files = os.listdir(good_path)\n",
    "    store_X(list_of_files, X_list)\n",
    "    convert_X_to_ndarray(X, X_list)\n",
    "    X_good_features = X\n",
    "    y_split_1 = len(X_good_features)\n",
    "    \n",
    "    #get the bad squats or validation data\n",
    "    X_list = []\n",
    "    X = []\n",
    "    list_of_files = os.listdir(good_path)\n",
    "    store_X(list_of_files, X_list)\n",
    "    convert_X_to_ndarray(X, X_list)\n",
    "    X_bad_features = X\n",
    "    y_split_2 = len(X_good_features)\n",
    "    \n",
    "    #combine good and bad into ordered list so we know what's what\n",
    "    X_good_and_bad_together = X_good_features + X_bad_features\n",
    "    X_pad = pad_values(X_good_and_bad_together)\n",
    "    \n",
    "    #now make the y based on the x length\n",
    "    y_good = [1 for i in range(1,y_split_1+1)]\n",
    "    y_bad = [0 for i in range(1,y_split_2+1)]\n",
    "    y = np.expand_dims((np.array(y_good+y_bad)),1) \n",
    "    \n",
    "    return [X_pad,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------\n",
    "#HERE ARE THE INSTRUCTIONS FOR THE ABOVE FUNCTION\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just put in your your paths, example = \"../raw_data/CSV_Features_good/\"\n",
    "good_path = 'INSERT_PATH_IN_QUOTES'\n",
    "bad_path  = 'INSERT_PATH_IN_QUOTES'\n",
    "\n",
    "#X and y values will automatically store\n",
    "#IMPORTANT: you still have to scale the data \n",
    "X,y = temporary_preprocessing(good_path,bad_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------\n",
    "#OTHER NOTES\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a. you can also store the features from today in a single line of code \n",
    "    #1b. uncomment and run this line: X_features = np.expand_dims(X[:,:,33],2)\n",
    "#2. this could also work for train and val values\n",
    "#3. not just splitting the 80 training CSVs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------------------------------------------------------------------------\n",
    "#I was doing above manually like this below, don't delete this please just in case \n",
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOOD FEATURES\n",
    "\n",
    "path = \"../raw_data/CSV_Features_good/\"\n",
    "list_of_files = os.listdir(\"../raw_data/CSV_Features_good/\")\n",
    "X_list = []\n",
    "X = []\n",
    "X_pad = []\n",
    "\n",
    "store_X(list_of_files, X_list)\n",
    "convert_X_to_ndarray(X, X_list)\n",
    "X_good_features = X\n",
    "\n",
    "#BAD FEATURES\n",
    "\n",
    "path = \"../raw_data/CSV_Features_bad/\"\n",
    "list_of_files = os.listdir(\"../raw_data/CSV_Features_bad/\")\n",
    "X_list = []\n",
    "X = []\n",
    "X_pad = []\n",
    "\n",
    "store_X(list_of_files, X_list)\n",
    "convert_X_to_ndarray(X, X_list)\n",
    "X_bad_features = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----------------------------------------------------------------------------------------\n",
    "#THIS SECTION IS ME MANUALLY WORKING THROUGH THE CODE BEFORE I WROTE THE FUNCTION ABOVE\n",
    "#I accidentally deleted some of it \n",
    "#but I got the X_good_and_bad_together the same way as the function\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780.0\n",
      "630.0\n",
      "780.0\n",
      "630.0\n"
     ]
    }
   ],
   "source": [
    "#this will store them in order\n",
    "X_good_and_bad_together = X_good_features + X_bad_features\n",
    "\n",
    "#you can check to see if they are ordered properly by printing the indexes pre and post\n",
    "print(X_good_features[0][15][15])\n",
    "print(X_bad_features[0][15][15])\n",
    "print(X_good_and_bad_together[0][15][15])\n",
    "print(X_good_and_bad_together[40][15][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 176, 34)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we need to pad\n",
    "#I can't get the pad function to call properly so I do it manually \n",
    "X_padded = pad_sequences(X_good_and_bad_together, dtype='float32',padding='post',value=-1000)\n",
    "\n",
    "#check the shape\n",
    "#should be 34 columns, 80 rows, length set by padder\n",
    "X_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the last column from the arrays where the features are\n",
    "X_final = X_padded[:,:,33]\n",
    "\n",
    "#expand the dimensions so it can be trained by the rnn\n",
    "X_final = np.expand_dims(X_final,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 (80,)\n"
     ]
    }
   ],
   "source": [
    "#check the shape, should have 1 in the third dimension\n",
    "X_final.shape\n",
    "\n",
    "#now create the y variable \n",
    "#make the first half 1 and second half 0 according to how we added X_good and X_bad features together in the list \n",
    "y = np.full(80,1)\n",
    "y[40:] = 0 \n",
    "\n",
    "#check to make sure data is right\n",
    "print(y[39],y[40],y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now expand the dimensions of y to (80,1) for the model\n",
    "#I don't think this is necessary but me and jules did it during our ticket session\n",
    "y = np.expand_dims(y,axis=1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 176)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finally let's scale on the fly\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#X_scaled = scaler.fit_transform(X_final)\n",
    "\n",
    "#ERROR RECEIVED - ValueError: Found array with dim 3. StandardScaler expected <= 2. \n",
    "\n",
    "#well it looks like you can't scale a three dimensional array \n",
    "#so we have to scale it back\n",
    "#we can do this by using an indexing trick\n",
    "\n",
    "X_final = X_final[:, :, 0]\n",
    "X_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026414739 136.30002\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled[0][0],X_final[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 176, 1)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we can move the dimensions back to 3\n",
    "X_scaled = np.expand_dims(X_scaled, axis = 2)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "------------\n",
    "#FINAL NOTES\n",
    "------------\n",
    "\n",
    "#I'm using a simple RNN layar and I have been tweeking the parameters\n",
    "#we need to experiment with more models \n",
    "#I haven't been able to get good accuracy, but I did improve the accuracy by jacking up the layers\n",
    "#not above 50 percent though\n",
    "\n",
    "------------\n",
    "#TO DO LIST \n",
    "------------\n",
    "\n",
    "#Make a pipeline perhaps incorporating some of the functions above\n",
    "#however, since we have not had a thorough chance to test data / \n",
    "#we really can't determine which models would work best\n",
    "#in that sense the pipeline is lower priority\n",
    "#we should just be trying as many models as we can and then build up \n",
    "#I will admit that I dropped the ball and not getting a grid search set up today\n",
    "#I am working on that right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.SimpleRNN(500,input_dim=1, activation='relu'))\n",
    "model.add(layers.Dense(250, activation='relu'))\n",
    "model.add(layers.Dense(125, activation='relu'))\n",
    "model.add(layers.Dense(75, activation='relu'))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 176ms/step - loss: nan - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "#may import implement early stopping for later\n",
    "model_fit = model.fit(X,y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "2/2 [==============================] - 2s 498ms/step - loss: nan - accuracy: 0.3542 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 2/75\n",
      "2/2 [==============================] - 1s 394ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 3/75\n",
      "2/2 [==============================] - 1s 283ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 4/75\n",
      "2/2 [==============================] - 0s 265ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 5/75\n",
      "2/2 [==============================] - 1s 441ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 6/75\n",
      "2/2 [==============================] - 1s 297ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 7/75\n",
      "2/2 [==============================] - 0s 276ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 8/75\n",
      "2/2 [==============================] - 1s 271ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 9/75\n",
      "2/2 [==============================] - 1s 272ms/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 10/75\n",
      "2/2 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.3542 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 11/75\n",
      "2/2 [==============================] - 0s 268ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 12/75\n",
      "2/2 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 13/75\n",
      "2/2 [==============================] - 0s 249ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 14/75\n",
      "2/2 [==============================] - 0s 265ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 15/75\n",
      "2/2 [==============================] - 0s 270ms/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 16/75\n",
      "2/2 [==============================] - 0s 278ms/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 17/75\n",
      "2/2 [==============================] - 0s 282ms/step - loss: nan - accuracy: 0.3542 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 18/75\n",
      "2/2 [==============================] - 0s 282ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 19/75\n",
      "2/2 [==============================] - 0s 277ms/step - loss: nan - accuracy: 0.3542 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 20/75\n",
      "2/2 [==============================] - 0s 268ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 21/75\n",
      "2/2 [==============================] - 0s 265ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 22/75\n",
      "2/2 [==============================] - 0s 266ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 23/75\n",
      "2/2 [==============================] - 0s 268ms/step - loss: nan - accuracy: 0.3542 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 24/75\n",
      "2/2 [==============================] - 0s 278ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 25/75\n",
      "2/2 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 26/75\n",
      "2/2 [==============================] - 0s 257ms/step - loss: nan - accuracy: 0.4062 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 27/75\n",
      "2/2 [==============================] - 0s 266ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 28/75\n",
      "2/2 [==============================] - 0s 274ms/step - loss: nan - accuracy: 0.4167 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 29/75\n",
      "2/2 [==============================] - 1s 287ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 30/75\n",
      "2/2 [==============================] - 1s 295ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 31/75\n",
      "2/2 [==============================] - 1s 298ms/step - loss: nan - accuracy: 0.4062 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 32/75\n",
      "2/2 [==============================] - 1s 302ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 33/75\n",
      "2/2 [==============================] - 1s 345ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 34/75\n",
      "2/2 [==============================] - 1s 371ms/step - loss: nan - accuracy: 0.4062 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 35/75\n",
      "2/2 [==============================] - 1s 412ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 36/75\n",
      "2/2 [==============================] - 1s 328ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 37/75\n",
      "2/2 [==============================] - 1s 300ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 38/75\n",
      "2/2 [==============================] - 1s 471ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 39/75\n",
      "2/2 [==============================] - 1s 363ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 40/75\n",
      "2/2 [==============================] - 1s 378ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 41/75\n",
      "2/2 [==============================] - 1s 420ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 42/75\n",
      "2/2 [==============================] - 1s 338ms/step - loss: nan - accuracy: 0.3542 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 43/75\n",
      "2/2 [==============================] - 1s 413ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 44/75\n",
      "2/2 [==============================] - 1s 360ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 45/75\n",
      "2/2 [==============================] - 1s 374ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 46/75\n",
      "2/2 [==============================] - 1s 344ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 47/75\n",
      "2/2 [==============================] - 1s 341ms/step - loss: nan - accuracy: 0.4375 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 48/75\n",
      "2/2 [==============================] - 1s 435ms/step - loss: nan - accuracy: 0.4062 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 49/75\n",
      "2/2 [==============================] - 1s 307ms/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 50/75\n",
      "2/2 [==============================] - 1s 306ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 51/75\n",
      "2/2 [==============================] - 1s 319ms/step - loss: nan - accuracy: 0.4062 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 52/75\n",
      "2/2 [==============================] - 1s 334ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 53/75\n",
      "2/2 [==============================] - 1s 323ms/step - loss: nan - accuracy: 0.4062 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 54/75\n",
      "2/2 [==============================] - 1s 331ms/step - loss: nan - accuracy: 0.3125 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 55/75\n",
      "2/2 [==============================] - 1s 320ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 56/75\n",
      "2/2 [==============================] - 1s 333ms/step - loss: nan - accuracy: 0.3542 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 57/75\n",
      "2/2 [==============================] - 1s 355ms/step - loss: nan - accuracy: 0.3333 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 58/75\n",
      "2/2 [==============================] - 1s 313ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 59/75\n",
      "2/2 [==============================] - 1s 341ms/step - loss: nan - accuracy: 0.4167 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 60/75\n",
      "2/2 [==============================] - 1s 344ms/step - loss: nan - accuracy: 0.3646 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 61/75\n",
      "2/2 [==============================] - 1s 323ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/75\n",
      "2/2 [==============================] - 1s 316ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 63/75\n",
      "2/2 [==============================] - 1s 515ms/step - loss: nan - accuracy: 0.4062 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 64/75\n",
      "2/2 [==============================] - 1s 482ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 65/75\n",
      "2/2 [==============================] - 0s 315ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 66/75\n",
      "2/2 [==============================] - 0s 261ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 67/75\n",
      "2/2 [==============================] - 1s 368ms/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 68/75\n",
      "2/2 [==============================] - 1s 505ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 69/75\n",
      "2/2 [==============================] - 1s 320ms/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 70/75\n",
      "2/2 [==============================] - 0s 284ms/step - loss: nan - accuracy: 0.3958 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 71/75\n",
      "2/2 [==============================] - 0s 295ms/step - loss: nan - accuracy: 0.3750 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 72/75\n",
      "2/2 [==============================] - 0s 252ms/step - loss: nan - accuracy: 0.3333 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 73/75\n",
      "2/2 [==============================] - 0s 220ms/step - loss: nan - accuracy: 0.4167 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 74/75\n",
      "2/2 [==============================] - 0s 222ms/step - loss: nan - accuracy: 0.3542 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 75/75\n",
      "2/2 [==============================] - 0s 228ms/step - loss: nan - accuracy: 0.3854 - val_loss: nan - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14efd8ee0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y_new, \n",
    "          epochs=75, \n",
    "          batch_size=32, \n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------------------------------------\n",
    "#FINISH GRID SEARCH HERE AFTER BREAK, I'M HOPEFUL ROBYN!\n",
    "--------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
